{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Boiler Plate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T00:08:38.489981Z",
     "start_time": "2020-07-31T00:08:36.833654Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Unzipping data and moving it in right location\n",
    "#!p7zip -d ./data/all.7z\n",
    "#!mkdir data\n",
    "#!mv negative data/\n",
    "#!mv positive data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Importing Library and Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T00:08:57.847565Z",
     "start_time": "2020-07-31T00:08:38.517910Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#from audio import * ## Imporing FastAI Audio Library\n",
    "\n",
    "from fastai.data.all import *\n",
    "from fastai.vision.all import *\n",
    "\n",
    "from audiotransform import AudioTransform, SpectrogramConfig2, AudioConfig2, label_func\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "from tqdm import tqdm_notebook ## For progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T16:44:42.189549Z",
     "start_time": "2020-07-30T16:44:41.976139Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Helper functions for model evaluation\n",
    "\n",
    "## Taken from my cookbooks - https://github.com/aayushmnit/cookbook/blob/master/ml_classification.py\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    accuracy_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    classes,\n",
    "    normalize=False,\n",
    "    title=\"Confusion matrix\",\n",
    "    cmap=plt.cm.Blues,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print(\"Confusion matrix, without normalization\")\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = \".2f\" if normalize else \"d\"\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(\n",
    "            j,\n",
    "            i,\n",
    "            format(cm[i, j], fmt),\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This step just checks data and provide some summary statistics like sampling rate of different audio clips and length distribution of each waveFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your data\n",
    "data_folder = Path(\"./data/train/mldata/all/\")\n",
    "\n",
    "\n",
    "# Get a list of audio files\n",
    "audio_files = get_files(data_folder, extensions=['.wav',])  # adjust extensions as needed\n",
    "\n",
    "# # You can then explore the audio files with torchaudio\n",
    "# for audio_file in audio_files:\n",
    "#     wave, sr = torchaudio.load(audio_file)\n",
    "#     print(f\"Loaded {audio_file}, Sample Rate: {sr}, Waveform Shape: {wave.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "# Integrate with fastai's DataBlock (customized for your use case)\n",
    "# def label_func(f): return f.parent.name\n",
    "\n",
    "# audio_block = DataBlock(\n",
    "#     blocks=(AudioTransform, CategoryBlock),\n",
    "#     get_items=get_files,\n",
    "#     get_y=label_func,\n",
    "#     splitter=RandomSplitter(),\n",
    "#     item_tfms=[],\n",
    "#     batch_tfms=[]\n",
    "# )\n",
    "\n",
    "# dls = audio_block.dataloaders(data_folder, bs=1)\n",
    "\n",
    "# xb, yb = dls.one_batch()\n",
    "# print(xb.shape, yb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ## Defining path of modeling related data (Contains two folder positive and negative)\n",
    "# data_folder = Path(\"./data/train/mldata/all/\") \n",
    "# audios = AudioList.from_folder(data_folder)\n",
    "# len_dict = audios.stats(prec=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_config = SpectrogramConfig2()\n",
    "config = AudioConfig2(sg_cfg=sg_config)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T16:45:41.151825Z",
     "start_time": "2020-07-30T16:45:40.979298Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ## Definining Audio config needed to create on the fly mel spectograms\n",
    "# config = AudioConfig(standardize=False, \n",
    "#                      sg_cfg=SpectrogramConfig(\n",
    "#                          f_min=0.0,  ## Minimum frequency to Display\n",
    "#                          f_max=10000, ## Maximum Frequency to Display\n",
    "#                          hop_length=256,\n",
    "#                          n_fft=2560, ## Number of Samples for Fourier\n",
    "#                          n_mels=256, ## Mel bins\n",
    "#                          pad=0, \n",
    "#                          to_db_scale=True, ## Converting to DB sclae\n",
    "#                          top_db=100,  ## Top decible sound\n",
    "#                          win_length=None, \n",
    "#                          n_mfcc=20)\n",
    "#                     )\n",
    "# config.duration = 4000 ## 4 sec padding or snip\n",
    "# config.resample_to=20000 ## Every sample at 20000 frequency\n",
    "# config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**HyperParameter Cheat Sheet - Taken from [here](https://nbviewer.jupyter.org/github/mogwai/fastai_audio/blob/master/tutorials/01_Intro_to_Audio.ipynb)**\n",
    "- sample_rate, This is not the place to change this, you are just telling librosa what your sample rate is. Usually it is predetermined for you by your dataset but check the resampling section for more info on changing this.\n",
    "- fmin, minimum frequency to display in spectrogram, this should be low, anything 0-20 seems to work well\n",
    "- fmax, maximum frequency to display. This should generally be 1/2 of your sample rate, but can be set to 8000 for speech.\n",
    "- n_mels, How many mel bins to use, this will determine number of pixels tall your sg is. 64-128 are good defaults, but try various values, bigger isn't always better, test for your dataset. Some evidence suggests upscaling the image to a larger size is more effective than\n",
    "- n_fft, The number of samples you use each time you compute a Fourier Transform. This is the width of the window and hop_length is how much you move the window each step. Increasing n_fft will increase frequency (y-axis) resolution to a point, powers of 2 are faster. Also dependent somewhat on n_mels so 20*n_mels is a common value as less than this can produce empty mel bins (black horizontal lines on sg)\n",
    "- hop_length, the number of samples between successive frames of your sg. Determines width of image (# samples/hop = width in pixels). Good defaults really depend on dataset and the duration of audio your sg's represent (if they are longer, a larger hop is required to fit on a gpu, but you will be compressing the data). If you go too small, you can get blurring. Anything 64-512 can be good depending on context.\n",
    "- top_db, Distance between loudest and softest sound you want displayed in spectrogram. If you choose 50db, the brightest pixel will be 50db, and anything that is 50+db lower than that won't be displayed. 80-120 is good.\n",
    "- power, Honestly not entirely sure how this works. It's set to 1 for \"energy\" spectrogram and 2 for \"power\" spectrogram. An energy spectrogram is more detailed (less energy required to show up on the sg) but you don't generally have to worry about this because if you are converting to decibels (you'll do this almost always) it is factored out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This code creates a AudioDataLoader and split data in random 80/20 split and takes the label from the folder name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create Data Loader\n",
    "\n",
    "audio_transform = AudioTransform(config, mode='train')\n",
    "\n",
    "\n",
    "# Define your DataBlock\n",
    "audio_block = DataBlock(\n",
    "    blocks=(TransformBlock, CategoryBlock),\n",
    "    get_items=get_files,\n",
    "    get_x=audio_transform,\n",
    "    get_y=label_func,\n",
    "    splitter=RandomSplitter(),\n",
    "    item_tfms=[],\n",
    "    batch_tfms=[]\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "dls = audio_block.dataloaders(data_folder, bs=32)\n",
    "\n",
    "xb, yb = dls.one_batch()\n",
    "print(xb.shape, yb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ## Creating Data Loader\n",
    "# audios = AudioList.from_folder(data_folder, config=config).split_by_rand_pct(.2, seed=4).label_from_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This code creates a AudioDataBunch which apply defined transformations (In our case frequency masking) on the fly and provide input spectograms to the model in defined bactch size (64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ## Defining Transformation\n",
    "# tfms = None\n",
    "\n",
    "# ## Frequency masking:ON\n",
    "# tfms = get_spectro_transforms(mask_time=False, mask_freq=True, roll=False) \n",
    "\n",
    "# ## Creating a databunch\n",
    "# db = audios.transform(tfms).databunch(bs=64)\n",
    "\n",
    "# ## Let's insepect some data\n",
    "# db.show_batch(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Code below creates a ResNet18 model, removes the last 2 fully connected layer and then add new fully connected layers and load the pretrained weights from ImageNet Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=[accuracy]\n",
    "\n",
    "learn = Learner(dls,models.resnet18(), metrics = metrics).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ## Default learner is ResNet 18 \n",
    "# learn = audio_learner(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is key feature of FastAI library, this helps us find the ideal learning rate by running model on sample data to see how the accuracy progresses. Output of this step is a learning rate curve (Choose the learning rate where loss starts bumping again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Find ideal learning rate\n",
    "learn.lr_find()\n",
    "learn.recorder.plot_lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T00:23:09.362133Z",
     "start_time": "2020-07-31T00:23:09.135833Z"
    },
    "hidden": true
   },
   "source": [
    "Training model, two cool things to highlight - \n",
    "- **This model is getting trained using [1 cycle learning policy]**(https://arxiv.org/abs/1803.09820) which leads to faster conversion, Here is a [cool blog](https://towardsdatascience.com/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6) explaing the same if you are not a paper person\n",
    "- **Differential learning rate** - You want different learning rate for different layer of models. In transfer learning you don't want to change learning rate of early layers as fast as later layers in network. (The slice function allows us to pass that information in FastAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## 1-cycle learning (5 epochs and variable learning rate)\n",
    "learn.fit_one_cycle(20, slice(2e-3, 2e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "FastAI outputs the model training porgress per epoch, Note that the accuracy is only calculated on Validation set (20% holdout set created during creating AudioDatabunch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Find ideal learning rate\n",
    "learn.lr_find()\n",
    "learn.recorder.plot_lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## 1-cycle learning (5 epochs and variable learning rate)\n",
    "learn.fit_one_cycle(5, slice(1e-5, 1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Exporting the model\n",
    "learn.export('models/stg2-rn18.pkl')\n",
    "\n",
    "torch.save(learn.model.state_dict(), 'models/stg2-rn18.pt') # torch version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T00:29:54.507136Z",
     "start_time": "2020-07-31T00:29:54.302755Z"
    },
    "hidden": true
   },
   "source": [
    "With just 15 minutes of training we got our accuracy up to ~93.7% on 20% holdout set which was not used for training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A cool function in fastAI to plot different evaluation measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_model(mPath, mName=\"stg2-rn18.pkl\"):\n",
    "    if mName.endswith('.pkl'):\n",
    "        tmp = load_learner(os.path.join(mPath, mName))    \n",
    "    elif mName.endswith('.pt'):\n",
    "        import torch\n",
    "        # it is a pytorch model\n",
    "        checkpoint = torch.load(os.path.join(mPath, mName), map_location=torch.device('cpu'))\n",
    "\n",
    "        # create a dummy dataloader\n",
    "        metrics = [accuracy]\n",
    "        # loss_func = CrossEntropyLossFlat()\n",
    "\n",
    "        # write a dummy wave file\n",
    "        wave =np.zeros((20000))\n",
    "        scipy.io.wavfile.write(os.path.join(mPath,'dummy.wav'), 20000, wave)\n",
    "        testpath = Path(mPath)\n",
    "\n",
    "\n",
    "        spec_config = SpectrogramConfig2()\n",
    "        config = AudioConfig2(sg_cfg=spec_config)\n",
    "\n",
    "        audio_transform = AudioTransform(config, mode='test')\n",
    "\n",
    "\n",
    "        get_wav_files = lambda x: get_files(x, extensions=['.wav',])\n",
    "        # Define your DataBlock\n",
    "        audio_block = DataBlock(\n",
    "            blocks=(TransformBlock, CategoryBlock),\n",
    "            get_items=get_wav_files,\n",
    "            get_x=audio_transform,\n",
    "            get_y=label_func,\n",
    "            splitter=RandomSplitter(),\n",
    "            item_tfms=[],\n",
    "            batch_tfms=[]\n",
    "        )\n",
    "\n",
    "        dls = audio_block.dataloaders(testpath, bs=1)\n",
    "\n",
    "\n",
    "        tmp = Learner(dls,models.resnet18(), metrics = metrics)\n",
    "\n",
    "\n",
    "        # load the model\n",
    "        for n, p in checkpoint.items():\n",
    "            print(n, p.shape)\n",
    "        for n, p in tmp.model.state_dict().items():\n",
    "            print(n, p.shape)\n",
    "        tmp.model.load_state_dict(checkpoint, strict=True)\n",
    "    else:raise NotImplementedError(\"Only .pkl and .pt models are supported\")\n",
    "    return tmp\n",
    "\n",
    "\n",
    "\n",
    "learn = _load_model('/media/bnestor/easystore2/aifororcas-livesystem/ModelTraining/models', 'stg2-rn18.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix(figsize=(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Plot top losses help you plot 10 most wrong prediction by the model, this helps you listen/visualize the sound. This helps you understand where the model is not performing the best and provide key insights. As we can listen in below examples some of these audios don't contain Orca Call but the labeling process has marked them positive and some cases where model thinks there is a Orca call but nobody tagged it as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "interp.plot_top_losses(10, heatmap = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model Evaluation on testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Defining DataFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_data_folder = Path(\"./data/test/all/\")\n",
    "test_data_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Creating a AudioBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get a list of audio files\n",
    "audio_files = get_files(test_data_folder, extensions=['.wav',])  # adjust extensions as needed\n",
    "\n",
    "audio_transform = AudioTransform(config, mode='test')\n",
    "\n",
    "\n",
    "# Define your DataBlock\n",
    "audio_block = DataBlock(\n",
    "    blocks=(TransformBlock, CategoryBlock),\n",
    "    get_items=get_files,\n",
    "    get_x=audio_transform,\n",
    "    get_y=label_func,\n",
    "    splitter=RandomSplitter(),\n",
    "    item_tfms=[],\n",
    "    batch_tfms=[]\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "test_dls = audio_block.dataloaders(test_data_folder, bs=1)\n",
    "\n",
    "xb, yb = test_dls.one_batch()\n",
    "print(xb.shape, yb.shape)\n",
    "\n",
    "\n",
    "\n",
    "## Also extracting true labels\n",
    "true_value = pd.Series([item[1].cpu().data.numpy().squeeze().tolist() for item in test_dls.train_ds])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# test = AudioList.from_folder(test_data_folder, config=config).split_none().label_from_folder()\n",
    "# testdb = test.transform(tfms).databunch(bs=64)\n",
    "\n",
    "# ## Also extracting true labels\n",
    "# true_value = pd.Series(list(testdb.train_ds.y.items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Generating predictions : \n",
    "- **To-Do** - There should be a better way to batch scoring, write now we have to score 1 by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for item in tqdm_notebook([item[0] for item in test_dls.train_ds]):    \n",
    "    predictions.append(learn.predict(item)[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Calulating performance measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"AUC Score :{0:.2f} \\nF-1 Score :{1:.2f} \\nAccuracy Score :{2:.2f} \\nAveragePrecisionScore :{1:.2f}\".format(\n",
    "    roc_auc_score(true_value,pd.Series(predictions)), \n",
    "    f1_score(true_value,pd.Series(predictions)>0.5), \n",
    "    accuracy_score(true_value,pd.Series(predictions)>0.5),\n",
    "    average_precision_score(true_value,pd.Series(predictions) )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T00:36:17.351814Z",
     "start_time": "2020-07-31T00:36:17.123828Z"
    },
    "hidden": true
   },
   "source": [
    "Wohoo model seems to performing inline with our initial model training process on this test set. Let's plot a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(true_value, pd.Series(predictions)>0.5, classes=[\"No Orca\",\"Orca\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring for official evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = load_learner('models/stg2-rn18.pkl')\n",
    "# learn = load_learner(\"./data/train/mldata/all/models/\", 'stg2-rn18.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the 2 sec audio clips generated in Data prepration step for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_folder = Path(\"./data/test/OrcasoundLab07052019_Test/test2Sec/\")\n",
    "test_data_folder = Path(\"./data/test/all/\")\n",
    "\n",
    "\n",
    "\n",
    "# Get a list of audio files\n",
    "# audio_files = get_files(data_folder, extensions=['.wav',])  # adjust extensions as needed\n",
    "\n",
    "audio_transform = AudioTransform(config, mode='test')\n",
    "\n",
    "\n",
    "# Define your DataBlock\n",
    "audio_block = DataBlock(\n",
    "    blocks=(TransformBlock, CategoryBlock),\n",
    "    get_items=get_files,\n",
    "    get_x=audio_transform,\n",
    "    get_y=label_func,\n",
    "    splitter=RandomSplitter(),\n",
    "    item_tfms=[],\n",
    "    batch_tfms=[]\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "test_dls = audio_block.dataloaders(test_data_folder, bs=32)\n",
    "\n",
    "xb, yb = dls.one_batch()\n",
    "print(xb.shape, yb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_folder = Path(\"./data/test/OrcasoundLab07052019_Test/test2Sec/\")\n",
    "# tfms=None\n",
    "# test = AudioList.from_folder(test_data_folder, config=config).split_none().label_empty()\n",
    "# testdb = test.transform(tfms).databunch(bs=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runnning though model and generating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "pathList = [] \n",
    "# for item in tqdm_notebook(testdb.x):\n",
    "#     predictions.append(learn.predict(item)[2][1])\n",
    "#     pathList.append(str(item.path))\n",
    "\n",
    "\n",
    "\n",
    "for pathname, item in tqdm_notebook(zip(test_dls.items, [item[0] for item in test_dls.train_ds]), total=len(test_dls.items)):    \n",
    "    predictions.append(learn.predict(item)[2][1].cpu().data.numpy().tolist())\n",
    "    pathList.append(str(pathname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporing the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame({'FilePath': pathList, 'pred': predictions})\n",
    "prediction['FileName'] = prediction.FilePath.apply(lambda x: os.path.basename(x).split(\"-\")[0])\n",
    "prediction.loc[:,['FileName','pred']].to_csv('./test2Sec.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the predictions in standard evaluation format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T17:26:20.639759Z",
     "start_time": "2020-07-30T17:26:20.413024Z"
    }
   },
   "outputs": [],
   "source": [
    "## Load predictions\n",
    "test2secDF = pd.read_csv(\"./test2Sec.csv\") \n",
    "\n",
    "display(test2secDF)\n",
    "\n",
    "# ## Clean the predictions(it got converted in string)\n",
    "# test2secDF['pred'] = test2secDF.pred.apply(lambda x: float(x.split('(')[1].split(')')[0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T17:28:29.203097Z",
     "start_time": "2020-07-30T17:28:28.995955Z"
    }
   },
   "outputs": [],
   "source": [
    "## Extracting Start time from file name\n",
    "test2secDF['startTime'] = test2secDF.FileName.apply(lambda x: int(x.split('__')[1].split('.')[0].split('_')[0]))\n",
    "\n",
    "## Sorting the file based on startTime\n",
    "test2secDF = test2secDF.sort_values(['startTime']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T17:47:41.863843Z",
     "start_time": "2020-07-30T17:47:41.661774Z"
    }
   },
   "outputs": [],
   "source": [
    "test2secDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T17:54:51.232979Z",
     "start_time": "2020-07-30T17:54:51.040092Z"
    }
   },
   "outputs": [],
   "source": [
    "## Rolling Window (to average at per second level)\n",
    "submission = pd.DataFrame({'pred': list(test2secDF.rolling(2)['pred'].mean().values)}).reset_index().rename(columns={'index':'StartTime'})\n",
    "\n",
    "## Updating first row\n",
    "submission.loc[0,'pred'] = test2secDF.pred[0]\n",
    "\n",
    "## Adding lastrow\n",
    "lastLine = pd.DataFrame({'StartTime':[submission.StartTime.max()+1],'pred':[test2secDF.pred[test2secDF.shape[0]-1]]})\n",
    "# submission = submission.append(lastLine, ignore_index=True)\n",
    "# display(lastLine)\n",
    "submission = pd.concat((submission, lastLine), ignore_index=True)\n",
    "# display(submission)\n",
    "\n",
    "finalSubmission = submission.loc[submission.pred > 0.5,:].reset_index(drop=True)\n",
    "finalSubmission['Duration'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T17:55:37.716590Z",
     "start_time": "2020-07-30T17:55:37.436363Z"
    }
   },
   "outputs": [],
   "source": [
    "## Final submission file\n",
    "finalSubmission.loc[:,['StartTime','Duration']].to_csv('../evaluation/submission/submission2SecFastAI.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
